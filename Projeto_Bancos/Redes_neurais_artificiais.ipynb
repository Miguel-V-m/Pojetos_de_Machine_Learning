{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36c361b9",
   "metadata": {},
   "source": [
    "# Redes neurais artificiais e inrodução a deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da5a6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db67d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819412a",
   "metadata": {},
   "source": [
    "## Base credit data - 99.80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a35e761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('credit.pkl', 'rb') as f:  \n",
    "  X_credit_treinamento, y_credit_treinamento, X_credit_teste, y_credit_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdd13583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_treinamento.shape, y_credit_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4386857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((500, 3), (500,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_credit_teste.shape, y_credit_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6a6e1a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(3 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e5f8a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.47861195\n",
      "Iteration 2, loss = 0.44732293\n",
      "Iteration 3, loss = 0.42182255\n",
      "Iteration 4, loss = 0.40096105\n",
      "Iteration 5, loss = 0.38239951\n",
      "Iteration 6, loss = 0.36499345\n",
      "Iteration 7, loss = 0.34819935\n",
      "Iteration 8, loss = 0.33122100\n",
      "Iteration 9, loss = 0.31448260\n",
      "Iteration 10, loss = 0.29796693\n",
      "Iteration 11, loss = 0.28142854\n",
      "Iteration 12, loss = 0.26509739\n",
      "Iteration 13, loss = 0.24899143\n",
      "Iteration 14, loss = 0.23265072\n",
      "Iteration 15, loss = 0.21731821\n",
      "Iteration 16, loss = 0.20313306\n",
      "Iteration 17, loss = 0.19010651\n",
      "Iteration 18, loss = 0.17806272\n",
      "Iteration 19, loss = 0.16746218\n",
      "Iteration 20, loss = 0.15754318\n",
      "Iteration 21, loss = 0.14900856\n",
      "Iteration 22, loss = 0.14128350\n",
      "Iteration 23, loss = 0.13433685\n",
      "Iteration 24, loss = 0.12790543\n",
      "Iteration 25, loss = 0.12221073\n",
      "Iteration 26, loss = 0.11702715\n",
      "Iteration 27, loss = 0.11222911\n",
      "Iteration 28, loss = 0.10782271\n",
      "Iteration 29, loss = 0.10402638\n",
      "Iteration 30, loss = 0.10046856\n",
      "Iteration 31, loss = 0.09697554\n",
      "Iteration 32, loss = 0.09390072\n",
      "Iteration 33, loss = 0.09083265\n",
      "Iteration 34, loss = 0.08811998\n",
      "Iteration 35, loss = 0.08563488\n",
      "Iteration 36, loss = 0.08328854\n",
      "Iteration 37, loss = 0.08107610\n",
      "Iteration 38, loss = 0.07896208\n",
      "Iteration 39, loss = 0.07699974\n",
      "Iteration 40, loss = 0.07509519\n",
      "Iteration 41, loss = 0.07332755\n",
      "Iteration 42, loss = 0.07169149\n",
      "Iteration 43, loss = 0.07008788\n",
      "Iteration 44, loss = 0.06846482\n",
      "Iteration 45, loss = 0.06700660\n",
      "Iteration 46, loss = 0.06556707\n",
      "Iteration 47, loss = 0.06420301\n",
      "Iteration 48, loss = 0.06323116\n",
      "Iteration 49, loss = 0.06163042\n",
      "Iteration 50, loss = 0.06057448\n",
      "Iteration 51, loss = 0.05934286\n",
      "Iteration 52, loss = 0.05827264\n",
      "Iteration 53, loss = 0.05711343\n",
      "Iteration 54, loss = 0.05604133\n",
      "Iteration 55, loss = 0.05504950\n",
      "Iteration 56, loss = 0.05405831\n",
      "Iteration 57, loss = 0.05320792\n",
      "Iteration 58, loss = 0.05220401\n",
      "Iteration 59, loss = 0.05134898\n",
      "Iteration 60, loss = 0.05046222\n",
      "Iteration 61, loss = 0.04971022\n",
      "Iteration 62, loss = 0.04881403\n",
      "Iteration 63, loss = 0.04799840\n",
      "Iteration 64, loss = 0.04734754\n",
      "Iteration 65, loss = 0.04658349\n",
      "Iteration 66, loss = 0.04583296\n",
      "Iteration 67, loss = 0.04504838\n",
      "Iteration 68, loss = 0.04474495\n",
      "Iteration 69, loss = 0.04375318\n",
      "Iteration 70, loss = 0.04317150\n",
      "Iteration 71, loss = 0.04274255\n",
      "Iteration 72, loss = 0.04192423\n",
      "Iteration 73, loss = 0.04132243\n",
      "Iteration 74, loss = 0.04085389\n",
      "Iteration 75, loss = 0.04051688\n",
      "Iteration 76, loss = 0.03966350\n",
      "Iteration 77, loss = 0.03911691\n",
      "Iteration 78, loss = 0.03869300\n",
      "Iteration 79, loss = 0.03807432\n",
      "Iteration 80, loss = 0.03766523\n",
      "Iteration 81, loss = 0.03721535\n",
      "Iteration 82, loss = 0.03670480\n",
      "Iteration 83, loss = 0.03628925\n",
      "Iteration 84, loss = 0.03591441\n",
      "Iteration 85, loss = 0.03536325\n",
      "Iteration 86, loss = 0.03495320\n",
      "Iteration 87, loss = 0.03456349\n",
      "Iteration 88, loss = 0.03417051\n",
      "Iteration 89, loss = 0.03383814\n",
      "Iteration 90, loss = 0.03329819\n",
      "Iteration 91, loss = 0.03307234\n",
      "Iteration 92, loss = 0.03264741\n",
      "Iteration 93, loss = 0.03218976\n",
      "Iteration 94, loss = 0.03188076\n",
      "Iteration 95, loss = 0.03154249\n",
      "Iteration 96, loss = 0.03104662\n",
      "Iteration 97, loss = 0.03082728\n",
      "Iteration 98, loss = 0.03040402\n",
      "Iteration 99, loss = 0.02996878\n",
      "Iteration 100, loss = 0.02988795\n",
      "Iteration 101, loss = 0.02924097\n",
      "Iteration 102, loss = 0.02902259\n",
      "Iteration 103, loss = 0.02867882\n",
      "Iteration 104, loss = 0.02829108\n",
      "Iteration 105, loss = 0.02792471\n",
      "Iteration 106, loss = 0.02771661\n",
      "Iteration 107, loss = 0.02739027\n",
      "Iteration 108, loss = 0.02695328\n",
      "Iteration 109, loss = 0.02665053\n",
      "Iteration 110, loss = 0.02641431\n",
      "Iteration 111, loss = 0.02614841\n",
      "Iteration 112, loss = 0.02645153\n",
      "Iteration 113, loss = 0.02562842\n",
      "Iteration 114, loss = 0.02558392\n",
      "Iteration 115, loss = 0.02548544\n",
      "Iteration 116, loss = 0.02483025\n",
      "Iteration 117, loss = 0.02456483\n",
      "Iteration 118, loss = 0.02432183\n",
      "Iteration 119, loss = 0.02406774\n",
      "Iteration 120, loss = 0.02381653\n",
      "Iteration 121, loss = 0.02357716\n",
      "Iteration 122, loss = 0.02352004\n",
      "Iteration 123, loss = 0.02328593\n",
      "Iteration 124, loss = 0.02290551\n",
      "Iteration 125, loss = 0.02276033\n",
      "Iteration 126, loss = 0.02242099\n",
      "Iteration 127, loss = 0.02223731\n",
      "Iteration 128, loss = 0.02216376\n",
      "Iteration 129, loss = 0.02187302\n",
      "Iteration 130, loss = 0.02179790\n",
      "Iteration 131, loss = 0.02144753\n",
      "Iteration 132, loss = 0.02131551\n",
      "Iteration 133, loss = 0.02107133\n",
      "Iteration 134, loss = 0.02094196\n",
      "Iteration 135, loss = 0.02070907\n",
      "Iteration 136, loss = 0.02053707\n",
      "Iteration 137, loss = 0.02040292\n",
      "Iteration 138, loss = 0.02019331\n",
      "Iteration 139, loss = 0.02006158\n",
      "Iteration 140, loss = 0.01974444\n",
      "Iteration 141, loss = 0.01977357\n",
      "Iteration 142, loss = 0.01969304\n",
      "Iteration 143, loss = 0.01931673\n",
      "Iteration 144, loss = 0.01925990\n",
      "Iteration 145, loss = 0.01918028\n",
      "Iteration 146, loss = 0.01904295\n",
      "Iteration 147, loss = 0.01876611\n",
      "Iteration 148, loss = 0.01857830\n",
      "Iteration 149, loss = 0.01853897\n",
      "Iteration 150, loss = 0.01838745\n",
      "Iteration 151, loss = 0.01814960\n",
      "Iteration 152, loss = 0.01802483\n",
      "Iteration 153, loss = 0.01784148\n",
      "Iteration 154, loss = 0.01797137\n",
      "Iteration 155, loss = 0.01766342\n",
      "Iteration 156, loss = 0.01747011\n",
      "Iteration 157, loss = 0.01726859\n",
      "Iteration 158, loss = 0.01719952\n",
      "Iteration 159, loss = 0.01708864\n",
      "Iteration 160, loss = 0.01691775\n",
      "Iteration 161, loss = 0.01676355\n",
      "Iteration 162, loss = 0.01670766\n",
      "Iteration 163, loss = 0.01659147\n",
      "Iteration 164, loss = 0.01650156\n",
      "Iteration 165, loss = 0.01655750\n",
      "Iteration 166, loss = 0.01639353\n",
      "Iteration 167, loss = 0.01617479\n",
      "Iteration 168, loss = 0.01607006\n",
      "Iteration 169, loss = 0.01591713\n",
      "Iteration 170, loss = 0.01581756\n",
      "Iteration 171, loss = 0.01584929\n",
      "Iteration 172, loss = 0.01573167\n",
      "Iteration 173, loss = 0.01557608\n",
      "Iteration 174, loss = 0.01550317\n",
      "Iteration 175, loss = 0.01525097\n",
      "Iteration 176, loss = 0.01523420\n",
      "Iteration 177, loss = 0.01500992\n",
      "Iteration 178, loss = 0.01529209\n",
      "Iteration 179, loss = 0.01473872\n",
      "Iteration 180, loss = 0.01482869\n",
      "Iteration 181, loss = 0.01460939\n",
      "Iteration 182, loss = 0.01490898\n",
      "Iteration 183, loss = 0.01447265\n",
      "Iteration 184, loss = 0.01445422\n",
      "Iteration 185, loss = 0.01417797\n",
      "Iteration 186, loss = 0.01426748\n",
      "Iteration 187, loss = 0.01408841\n",
      "Iteration 188, loss = 0.01405840\n",
      "Iteration 189, loss = 0.01389188\n",
      "Iteration 190, loss = 0.01379868\n",
      "Iteration 191, loss = 0.01380523\n",
      "Iteration 192, loss = 0.01359517\n",
      "Iteration 193, loss = 0.01367908\n",
      "Iteration 194, loss = 0.01340748\n",
      "Iteration 195, loss = 0.01332167\n",
      "Iteration 196, loss = 0.01328014\n",
      "Iteration 197, loss = 0.01318041\n",
      "Iteration 198, loss = 0.01316818\n",
      "Iteration 199, loss = 0.01304530\n",
      "Iteration 200, loss = 0.01298367\n",
      "Iteration 201, loss = 0.01284597\n",
      "Iteration 202, loss = 0.01284563\n",
      "Iteration 203, loss = 0.01293871\n",
      "Iteration 204, loss = 0.01266855\n",
      "Iteration 205, loss = 0.01274912\n",
      "Iteration 206, loss = 0.01264710\n",
      "Iteration 207, loss = 0.01232228\n",
      "Iteration 208, loss = 0.01235829\n",
      "Iteration 209, loss = 0.01222224\n",
      "Iteration 210, loss = 0.01224567\n",
      "Iteration 211, loss = 0.01206861\n",
      "Iteration 212, loss = 0.01211891\n",
      "Iteration 213, loss = 0.01271595\n",
      "Iteration 214, loss = 0.01173725\n",
      "Iteration 215, loss = 0.01204173\n",
      "Iteration 216, loss = 0.01180451\n",
      "Iteration 217, loss = 0.01163137\n",
      "Iteration 218, loss = 0.01181789\n",
      "Iteration 219, loss = 0.01159620\n",
      "Iteration 220, loss = 0.01150827\n",
      "Iteration 221, loss = 0.01166800\n",
      "Iteration 222, loss = 0.01166050\n",
      "Iteration 223, loss = 0.01142429\n",
      "Iteration 224, loss = 0.01121973\n",
      "Iteration 225, loss = 0.01114376\n",
      "Iteration 226, loss = 0.01107752\n",
      "Iteration 227, loss = 0.01106952\n",
      "Iteration 228, loss = 0.01096934\n",
      "Iteration 229, loss = 0.01091361\n",
      "Iteration 230, loss = 0.01084941\n",
      "Iteration 231, loss = 0.01079334\n",
      "Iteration 232, loss = 0.01069807\n",
      "Iteration 233, loss = 0.01073510\n",
      "Iteration 234, loss = 0.01058139\n",
      "Iteration 235, loss = 0.01051642\n",
      "Iteration 236, loss = 0.01054256\n",
      "Iteration 237, loss = 0.01048659\n",
      "Iteration 238, loss = 0.01052458\n",
      "Iteration 239, loss = 0.01043319\n",
      "Iteration 240, loss = 0.01029113\n",
      "Iteration 241, loss = 0.01045117\n",
      "Iteration 242, loss = 0.01015826\n",
      "Iteration 243, loss = 0.01017814\n",
      "Iteration 244, loss = 0.01043572\n",
      "Iteration 245, loss = 0.01015720\n",
      "Iteration 246, loss = 0.01001299\n",
      "Iteration 247, loss = 0.00992168\n",
      "Iteration 248, loss = 0.00997040\n",
      "Iteration 249, loss = 0.00985149\n",
      "Iteration 250, loss = 0.00973461\n",
      "Iteration 251, loss = 0.00971935\n",
      "Iteration 252, loss = 0.00961514\n",
      "Iteration 253, loss = 0.00956827\n",
      "Iteration 254, loss = 0.00954919\n",
      "Iteration 255, loss = 0.00965749\n",
      "Iteration 256, loss = 0.00941693\n",
      "Iteration 257, loss = 0.00939302\n",
      "Iteration 258, loss = 0.00930102\n",
      "Iteration 259, loss = 0.00927567\n",
      "Iteration 260, loss = 0.00918179\n",
      "Iteration 261, loss = 0.00922742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 262, loss = 0.00915427\n",
      "Iteration 263, loss = 0.00910844\n",
      "Iteration 264, loss = 0.00899680\n",
      "Iteration 265, loss = 0.00907150\n",
      "Iteration 266, loss = 0.00891218\n",
      "Iteration 267, loss = 0.00891122\n",
      "Iteration 268, loss = 0.00884635\n",
      "Iteration 269, loss = 0.00883616\n",
      "Iteration 270, loss = 0.00897609\n",
      "Iteration 271, loss = 0.00884409\n",
      "Iteration 272, loss = 0.00870553\n",
      "Iteration 273, loss = 0.00861298\n",
      "Iteration 274, loss = 0.00864378\n",
      "Iteration 275, loss = 0.00854898\n",
      "Iteration 276, loss = 0.00858478\n",
      "Iteration 277, loss = 0.00855732\n",
      "Iteration 278, loss = 0.00846750\n",
      "Iteration 279, loss = 0.00839685\n",
      "Iteration 280, loss = 0.00840761\n",
      "Iteration 281, loss = 0.00844212\n",
      "Iteration 282, loss = 0.00834651\n",
      "Iteration 283, loss = 0.00824530\n",
      "Iteration 284, loss = 0.00842423\n",
      "Iteration 285, loss = 0.00809650\n",
      "Iteration 286, loss = 0.00818588\n",
      "Iteration 287, loss = 0.00809182\n",
      "Iteration 288, loss = 0.00801151\n",
      "Iteration 289, loss = 0.00817876\n",
      "Iteration 290, loss = 0.00797640\n",
      "Iteration 291, loss = 0.00795412\n",
      "Iteration 292, loss = 0.00795239\n",
      "Iteration 293, loss = 0.00783827\n",
      "Iteration 294, loss = 0.00784825\n",
      "Iteration 295, loss = 0.00776394\n",
      "Iteration 296, loss = 0.00768421\n",
      "Iteration 297, loss = 0.00764572\n",
      "Iteration 298, loss = 0.00765861\n",
      "Iteration 299, loss = 0.00768045\n",
      "Iteration 300, loss = 0.00755301\n",
      "Iteration 301, loss = 0.00752246\n",
      "Iteration 302, loss = 0.00751216\n",
      "Iteration 303, loss = 0.00744665\n",
      "Iteration 304, loss = 0.00739937\n",
      "Iteration 305, loss = 0.00738096\n",
      "Iteration 306, loss = 0.00746795\n",
      "Iteration 307, loss = 0.00744026\n",
      "Iteration 308, loss = 0.00728719\n",
      "Iteration 309, loss = 0.00721463\n",
      "Iteration 310, loss = 0.00720325\n",
      "Iteration 311, loss = 0.00719264\n",
      "Iteration 312, loss = 0.00715189\n",
      "Iteration 313, loss = 0.00709063\n",
      "Iteration 314, loss = 0.00714169\n",
      "Iteration 315, loss = 0.00700671\n",
      "Iteration 316, loss = 0.00699210\n",
      "Iteration 317, loss = 0.00697503\n",
      "Iteration 318, loss = 0.00696242\n",
      "Iteration 319, loss = 0.00690284\n",
      "Iteration 320, loss = 0.00687605\n",
      "Iteration 321, loss = 0.00693225\n",
      "Iteration 322, loss = 0.00684952\n",
      "Iteration 323, loss = 0.00691947\n",
      "Iteration 324, loss = 0.00682268\n",
      "Iteration 325, loss = 0.00698598\n",
      "Iteration 326, loss = 0.00651291\n",
      "Iteration 327, loss = 0.00689181\n",
      "Iteration 328, loss = 0.00662790\n",
      "Iteration 329, loss = 0.00659455\n",
      "Iteration 330, loss = 0.00655223\n",
      "Iteration 331, loss = 0.00656084\n",
      "Iteration 332, loss = 0.00681746\n",
      "Iteration 333, loss = 0.00643435\n",
      "Iteration 334, loss = 0.00651609\n",
      "Iteration 335, loss = 0.00644754\n",
      "Iteration 336, loss = 0.00642998\n",
      "Iteration 337, loss = 0.00630583\n",
      "Iteration 338, loss = 0.00636302\n",
      "Iteration 339, loss = 0.00640276\n",
      "Iteration 340, loss = 0.00635082\n",
      "Iteration 341, loss = 0.00627795\n",
      "Iteration 342, loss = 0.00626596\n",
      "Iteration 343, loss = 0.00616946\n",
      "Iteration 344, loss = 0.00621834\n",
      "Iteration 345, loss = 0.00617895\n",
      "Iteration 346, loss = 0.00611881\n",
      "Iteration 347, loss = 0.00611198\n",
      "Iteration 348, loss = 0.00606313\n",
      "Iteration 349, loss = 0.00600431\n",
      "Iteration 350, loss = 0.00598821\n",
      "Iteration 351, loss = 0.00606050\n",
      "Iteration 352, loss = 0.00601773\n",
      "Iteration 353, loss = 0.00596146\n",
      "Iteration 354, loss = 0.00587165\n",
      "Iteration 355, loss = 0.00591310\n",
      "Iteration 356, loss = 0.00594532\n",
      "Iteration 357, loss = 0.00575754\n",
      "Iteration 358, loss = 0.00583367\n",
      "Iteration 359, loss = 0.00574578\n",
      "Iteration 360, loss = 0.00572078\n",
      "Iteration 361, loss = 0.00574197\n",
      "Iteration 362, loss = 0.00571107\n",
      "Iteration 363, loss = 0.00568071\n",
      "Iteration 364, loss = 0.00570555\n",
      "Iteration 365, loss = 0.00571200\n",
      "Iteration 366, loss = 0.00565704\n",
      "Iteration 367, loss = 0.00563157\n",
      "Iteration 368, loss = 0.00575912\n",
      "Iteration 369, loss = 0.00582777\n",
      "Iteration 370, loss = 0.00558485\n",
      "Iteration 371, loss = 0.00576962\n",
      "Iteration 372, loss = 0.00552931\n",
      "Iteration 373, loss = 0.00561662\n",
      "Iteration 374, loss = 0.00539332\n",
      "Iteration 375, loss = 0.00544748\n",
      "Iteration 376, loss = 0.00537339\n",
      "Iteration 377, loss = 0.00548211\n",
      "Iteration 378, loss = 0.00525509\n",
      "Iteration 379, loss = 0.00549176\n",
      "Iteration 380, loss = 0.00529360\n",
      "Iteration 381, loss = 0.00529620\n",
      "Iteration 382, loss = 0.00526256\n",
      "Iteration 383, loss = 0.00521109\n",
      "Iteration 384, loss = 0.00525516\n",
      "Iteration 385, loss = 0.00526848\n",
      "Iteration 386, loss = 0.00520375\n",
      "Iteration 387, loss = 0.00517678\n",
      "Iteration 388, loss = 0.00512256\n",
      "Iteration 389, loss = 0.00523203\n",
      "Iteration 390, loss = 0.00515146\n",
      "Iteration 391, loss = 0.00506920\n",
      "Iteration 392, loss = 0.00514573\n",
      "Iteration 393, loss = 0.00499964\n",
      "Iteration 394, loss = 0.00511621\n",
      "Iteration 395, loss = 0.00503245\n",
      "Iteration 396, loss = 0.00500316\n",
      "Iteration 397, loss = 0.00492680\n",
      "Iteration 398, loss = 0.00491734\n",
      "Iteration 399, loss = 0.00498952\n",
      "Iteration 400, loss = 0.00486364\n",
      "Iteration 401, loss = 0.00481587\n",
      "Iteration 402, loss = 0.00481406\n",
      "Iteration 403, loss = 0.00477639\n",
      "Iteration 404, loss = 0.00482440\n",
      "Iteration 405, loss = 0.00476003\n",
      "Iteration 406, loss = 0.00479686\n",
      "Iteration 407, loss = 0.00485684\n",
      "Iteration 408, loss = 0.00477813\n",
      "Iteration 409, loss = 0.00467986\n",
      "Iteration 410, loss = 0.00472029\n",
      "Iteration 411, loss = 0.00475663\n",
      "Iteration 412, loss = 0.00465461\n",
      "Iteration 413, loss = 0.00462090\n",
      "Iteration 414, loss = 0.00465083\n",
      "Iteration 415, loss = 0.00458598\n",
      "Iteration 416, loss = 0.00461212\n",
      "Iteration 417, loss = 0.00455310\n",
      "Iteration 418, loss = 0.00454922\n",
      "Iteration 419, loss = 0.00450498\n",
      "Iteration 420, loss = 0.00445795\n",
      "Iteration 421, loss = 0.00461382\n",
      "Iteration 422, loss = 0.00449162\n",
      "Iteration 423, loss = 0.00439819\n",
      "Iteration 424, loss = 0.00443461\n",
      "Iteration 425, loss = 0.00440178\n",
      "Iteration 426, loss = 0.00451129\n",
      "Iteration 427, loss = 0.00473559\n",
      "Iteration 428, loss = 0.00450633\n",
      "Iteration 429, loss = 0.00440149\n",
      "Iteration 430, loss = 0.00429621\n",
      "Iteration 431, loss = 0.00435651\n",
      "Iteration 432, loss = 0.00433199\n",
      "Iteration 433, loss = 0.00422934\n",
      "Iteration 434, loss = 0.00421319\n",
      "Iteration 435, loss = 0.00423849\n",
      "Iteration 436, loss = 0.00416890\n",
      "Iteration 437, loss = 0.00431232\n",
      "Iteration 438, loss = 0.00411898\n",
      "Iteration 439, loss = 0.00416605\n",
      "Iteration 440, loss = 0.00415562\n",
      "Iteration 441, loss = 0.00414095\n",
      "Iteration 442, loss = 0.00412555\n",
      "Iteration 443, loss = 0.00405962\n",
      "Iteration 444, loss = 0.00406892\n",
      "Iteration 445, loss = 0.00410931\n",
      "Iteration 446, loss = 0.00410356\n",
      "Iteration 447, loss = 0.00427092\n",
      "Iteration 448, loss = 0.00401713\n",
      "Iteration 449, loss = 0.00408843\n",
      "Iteration 450, loss = 0.00396151\n",
      "Iteration 451, loss = 0.00407413\n",
      "Iteration 452, loss = 0.00396836\n",
      "Iteration 453, loss = 0.00394553\n",
      "Iteration 454, loss = 0.00417973\n",
      "Iteration 455, loss = 0.00382948\n",
      "Iteration 456, loss = 0.00397583\n",
      "Iteration 457, loss = 0.00386793\n",
      "Iteration 458, loss = 0.00396844\n",
      "Iteration 459, loss = 0.00379736\n",
      "Iteration 460, loss = 0.00399644\n",
      "Iteration 461, loss = 0.00454090\n",
      "Iteration 462, loss = 0.00393179\n",
      "Iteration 463, loss = 0.00409567\n",
      "Iteration 464, loss = 0.00391411\n",
      "Iteration 465, loss = 0.00373134\n",
      "Iteration 466, loss = 0.00377762\n",
      "Iteration 467, loss = 0.00372957\n",
      "Iteration 468, loss = 0.00366938\n",
      "Iteration 469, loss = 0.00369293\n",
      "Iteration 470, loss = 0.00367150\n",
      "Iteration 471, loss = 0.00369706\n",
      "Iteration 472, loss = 0.00391286\n",
      "Iteration 473, loss = 0.00358906\n",
      "Iteration 474, loss = 0.00372121\n",
      "Iteration 475, loss = 0.00367990\n",
      "Iteration 476, loss = 0.00362802\n",
      "Iteration 477, loss = 0.00362412\n",
      "Iteration 478, loss = 0.00393625\n",
      "Iteration 479, loss = 0.00355960\n",
      "Iteration 480, loss = 0.00374457\n",
      "Iteration 481, loss = 0.00354373\n",
      "Iteration 482, loss = 0.00348050\n",
      "Iteration 483, loss = 0.00347608\n",
      "Iteration 484, loss = 0.00353739\n",
      "Iteration 485, loss = 0.00360176\n",
      "Iteration 486, loss = 0.00371978\n",
      "Iteration 487, loss = 0.00354367\n",
      "Iteration 488, loss = 0.00341377\n",
      "Iteration 489, loss = 0.00346673\n",
      "Iteration 490, loss = 0.00345481\n",
      "Iteration 491, loss = 0.00345599\n",
      "Iteration 492, loss = 0.00334654\n",
      "Iteration 493, loss = 0.00342975\n",
      "Iteration 494, loss = 0.00340867\n",
      "Iteration 495, loss = 0.00336474\n",
      "Iteration 496, loss = 0.00331230\n",
      "Iteration 497, loss = 0.00331962\n",
      "Iteration 498, loss = 0.00353647\n",
      "Iteration 499, loss = 0.00325232\n",
      "Iteration 500, loss = 0.00332764\n",
      "Iteration 501, loss = 0.00321207\n",
      "Iteration 502, loss = 0.00335408\n",
      "Iteration 503, loss = 0.00330280\n",
      "Iteration 504, loss = 0.00320987\n",
      "Iteration 505, loss = 0.00328220\n",
      "Iteration 506, loss = 0.00319162\n",
      "Iteration 507, loss = 0.00318989\n",
      "Iteration 508, loss = 0.00319949\n",
      "Iteration 509, loss = 0.00326646\n",
      "Iteration 510, loss = 0.00313689\n",
      "Iteration 511, loss = 0.00315988\n",
      "Iteration 512, loss = 0.00311573\n",
      "Iteration 513, loss = 0.00311717\n",
      "Iteration 514, loss = 0.00317828\n",
      "Iteration 515, loss = 0.00311778\n",
      "Iteration 516, loss = 0.00310002\n",
      "Iteration 517, loss = 0.00312555\n",
      "Iteration 518, loss = 0.00305916\n",
      "Iteration 519, loss = 0.00305365\n",
      "Iteration 520, loss = 0.00306462\n",
      "Iteration 521, loss = 0.00313061\n",
      "Iteration 522, loss = 0.00307231\n",
      "Iteration 523, loss = 0.00302837\n",
      "Iteration 524, loss = 0.00297732\n",
      "Iteration 525, loss = 0.00299902\n",
      "Iteration 526, loss = 0.00298245\n",
      "Iteration 527, loss = 0.00295667\n",
      "Iteration 528, loss = 0.00302392\n",
      "Iteration 529, loss = 0.00312082\n",
      "Iteration 530, loss = 0.00297533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 531, loss = 0.00299076\n",
      "Iteration 532, loss = 0.00292061\n",
      "Iteration 533, loss = 0.00289781\n",
      "Iteration 534, loss = 0.00284868\n",
      "Iteration 535, loss = 0.00289574\n",
      "Iteration 536, loss = 0.00313274\n",
      "Iteration 537, loss = 0.00283497\n",
      "Iteration 538, loss = 0.00300068\n",
      "Iteration 539, loss = 0.00298951\n",
      "Iteration 540, loss = 0.00285361\n",
      "Iteration 541, loss = 0.00283580\n",
      "Iteration 542, loss = 0.00280087\n",
      "Iteration 543, loss = 0.00282874\n",
      "Iteration 544, loss = 0.00277426\n",
      "Iteration 545, loss = 0.00282822\n",
      "Iteration 546, loss = 0.00277473\n",
      "Iteration 547, loss = 0.00277332\n",
      "Iteration 548, loss = 0.00272013\n",
      "Iteration 549, loss = 0.00274527\n",
      "Iteration 550, loss = 0.00272455\n",
      "Iteration 551, loss = 0.00282325\n",
      "Iteration 552, loss = 0.00264309\n",
      "Iteration 553, loss = 0.00286420\n",
      "Iteration 554, loss = 0.00280652\n",
      "Iteration 555, loss = 0.00265984\n",
      "Iteration 556, loss = 0.00264998\n",
      "Iteration 557, loss = 0.00262670\n",
      "Iteration 558, loss = 0.00265222\n",
      "Iteration 559, loss = 0.00264655\n",
      "Iteration 560, loss = 0.00272325\n",
      "Iteration 561, loss = 0.00284709\n",
      "Iteration 562, loss = 0.00268052\n",
      "Iteration 563, loss = 0.00258392\n",
      "Iteration 564, loss = 0.00256201\n",
      "Iteration 565, loss = 0.00261753\n",
      "Iteration 566, loss = 0.00255628\n",
      "Iteration 567, loss = 0.00258500\n",
      "Iteration 568, loss = 0.00256358\n",
      "Iteration 569, loss = 0.00251306\n",
      "Iteration 570, loss = 0.00259343\n",
      "Iteration 571, loss = 0.00251192\n",
      "Iteration 572, loss = 0.00254361\n",
      "Iteration 573, loss = 0.00260285\n",
      "Iteration 574, loss = 0.00265121\n",
      "Iteration 575, loss = 0.00247368\n",
      "Iteration 576, loss = 0.00247932\n",
      "Iteration 577, loss = 0.00254668\n",
      "Iteration 578, loss = 0.00247915\n",
      "Iteration 579, loss = 0.00248680\n",
      "Iteration 580, loss = 0.00252185\n",
      "Iteration 581, loss = 0.00245440\n",
      "Iteration 582, loss = 0.00240791\n",
      "Iteration 583, loss = 0.00243114\n",
      "Iteration 584, loss = 0.00238209\n",
      "Iteration 585, loss = 0.00253689\n",
      "Iteration 586, loss = 0.00262243\n",
      "Iteration 587, loss = 0.00245337\n",
      "Iteration 588, loss = 0.00238561\n",
      "Iteration 589, loss = 0.00237826\n",
      "Iteration 590, loss = 0.00236036\n",
      "Iteration 591, loss = 0.00239727\n",
      "Iteration 592, loss = 0.00230707\n",
      "Iteration 593, loss = 0.00232326\n",
      "Iteration 594, loss = 0.00236590\n",
      "Iteration 595, loss = 0.00240355\n",
      "Iteration 596, loss = 0.00239667\n",
      "Iteration 597, loss = 0.00243337\n",
      "Iteration 598, loss = 0.00226082\n",
      "Iteration 599, loss = 0.00235481\n",
      "Iteration 600, loss = 0.00226344\n",
      "Iteration 601, loss = 0.00224285\n",
      "Iteration 602, loss = 0.00226885\n",
      "Iteration 603, loss = 0.00222779\n",
      "Iteration 604, loss = 0.00228053\n",
      "Iteration 605, loss = 0.00222085\n",
      "Iteration 606, loss = 0.00220946\n",
      "Iteration 607, loss = 0.00225826\n",
      "Iteration 608, loss = 0.00215519\n",
      "Iteration 609, loss = 0.00223597\n",
      "Iteration 610, loss = 0.00221573\n",
      "Iteration 611, loss = 0.00222180\n",
      "Iteration 612, loss = 0.00216144\n",
      "Iteration 613, loss = 0.00218119\n",
      "Iteration 614, loss = 0.00216592\n",
      "Iteration 615, loss = 0.00216058\n",
      "Iteration 616, loss = 0.00217572\n",
      "Iteration 617, loss = 0.00210574\n",
      "Iteration 618, loss = 0.00216130\n",
      "Iteration 619, loss = 0.00219013\n",
      "Iteration 620, loss = 0.00211027\n",
      "Iteration 621, loss = 0.00212401\n",
      "Iteration 622, loss = 0.00209011\n",
      "Iteration 623, loss = 0.00207784\n",
      "Iteration 624, loss = 0.00205363\n",
      "Iteration 625, loss = 0.00218823\n",
      "Iteration 626, loss = 0.00211483\n",
      "Iteration 627, loss = 0.00206318\n",
      "Iteration 628, loss = 0.00213139\n",
      "Iteration 629, loss = 0.00215133\n",
      "Iteration 630, loss = 0.00202532\n",
      "Iteration 631, loss = 0.00208501\n",
      "Iteration 632, loss = 0.00198953\n",
      "Iteration 633, loss = 0.00205661\n",
      "Iteration 634, loss = 0.00195780\n",
      "Iteration 635, loss = 0.00201688\n",
      "Iteration 636, loss = 0.00204541\n",
      "Iteration 637, loss = 0.00210612\n",
      "Iteration 638, loss = 0.00208879\n",
      "Iteration 639, loss = 0.00209754\n",
      "Iteration 640, loss = 0.00198355\n",
      "Iteration 641, loss = 0.00194405\n",
      "Iteration 642, loss = 0.00199909\n",
      "Iteration 643, loss = 0.00192185\n",
      "Iteration 644, loss = 0.00196158\n",
      "Iteration 645, loss = 0.00191419\n",
      "Iteration 646, loss = 0.00198958\n",
      "Iteration 647, loss = 0.00192938\n",
      "Iteration 648, loss = 0.00190760\n",
      "Iteration 649, loss = 0.00191417\n",
      "Iteration 650, loss = 0.00189273\n",
      "Iteration 651, loss = 0.00189636\n",
      "Iteration 652, loss = 0.00189112\n",
      "Iteration 653, loss = 0.00187120\n",
      "Iteration 654, loss = 0.00185071\n",
      "Iteration 655, loss = 0.00183668\n",
      "Iteration 656, loss = 0.00190970\n",
      "Iteration 657, loss = 0.00183672\n",
      "Iteration 658, loss = 0.00184227\n",
      "Iteration 659, loss = 0.00182391\n",
      "Iteration 660, loss = 0.00180891\n",
      "Iteration 661, loss = 0.00181246\n",
      "Iteration 662, loss = 0.00185376\n",
      "Iteration 663, loss = 0.00182440\n",
      "Iteration 664, loss = 0.00180070\n",
      "Iteration 665, loss = 0.00177913\n",
      "Iteration 666, loss = 0.00179498\n",
      "Iteration 667, loss = 0.00179499\n",
      "Iteration 668, loss = 0.00177040\n",
      "Iteration 669, loss = 0.00178443\n",
      "Iteration 670, loss = 0.00173601\n",
      "Iteration 671, loss = 0.00179797\n",
      "Iteration 672, loss = 0.00175163\n",
      "Iteration 673, loss = 0.00171770\n",
      "Iteration 674, loss = 0.00172306\n",
      "Iteration 675, loss = 0.00182739\n",
      "Iteration 676, loss = 0.00170686\n",
      "Iteration 677, loss = 0.00180576\n",
      "Iteration 678, loss = 0.00184878\n",
      "Iteration 679, loss = 0.00170798\n",
      "Iteration 680, loss = 0.00185066\n",
      "Iteration 681, loss = 0.00165513\n",
      "Iteration 682, loss = 0.00183856\n",
      "Iteration 683, loss = 0.00168575\n",
      "Iteration 684, loss = 0.00170701\n",
      "Iteration 685, loss = 0.00162932\n",
      "Iteration 686, loss = 0.00172530\n",
      "Iteration 687, loss = 0.00166107\n",
      "Iteration 688, loss = 0.00171495\n",
      "Iteration 689, loss = 0.00162777\n",
      "Iteration 690, loss = 0.00168246\n",
      "Iteration 691, loss = 0.00163117\n",
      "Iteration 692, loss = 0.00165694\n",
      "Iteration 693, loss = 0.00161229\n",
      "Iteration 694, loss = 0.00159326\n",
      "Iteration 695, loss = 0.00161250\n",
      "Iteration 696, loss = 0.00161030\n",
      "Iteration 697, loss = 0.00160803\n",
      "Iteration 698, loss = 0.00158414\n",
      "Iteration 699, loss = 0.00159094\n",
      "Iteration 700, loss = 0.00163791\n",
      "Iteration 701, loss = 0.00163350\n",
      "Iteration 702, loss = 0.00161346\n",
      "Iteration 703, loss = 0.00156638\n",
      "Iteration 704, loss = 0.00157960\n",
      "Iteration 705, loss = 0.00158918\n",
      "Iteration 706, loss = 0.00155901\n",
      "Iteration 707, loss = 0.00154056\n",
      "Iteration 708, loss = 0.00154171\n",
      "Iteration 709, loss = 0.00151708\n",
      "Iteration 710, loss = 0.00158109\n",
      "Iteration 711, loss = 0.00161626\n",
      "Iteration 712, loss = 0.00155980\n",
      "Iteration 713, loss = 0.00150387\n",
      "Iteration 714, loss = 0.00155119\n",
      "Iteration 715, loss = 0.00149210\n",
      "Iteration 716, loss = 0.00154396\n",
      "Iteration 717, loss = 0.00148416\n",
      "Iteration 718, loss = 0.00146207\n",
      "Iteration 719, loss = 0.00148584\n",
      "Iteration 720, loss = 0.00148038\n",
      "Iteration 721, loss = 0.00145155\n",
      "Iteration 722, loss = 0.00143013\n",
      "Iteration 723, loss = 0.00146531\n",
      "Iteration 724, loss = 0.00146356\n",
      "Iteration 725, loss = 0.00145758\n",
      "Iteration 726, loss = 0.00146565\n",
      "Iteration 727, loss = 0.00146756\n",
      "Iteration 728, loss = 0.00146816\n",
      "Iteration 729, loss = 0.00142007\n",
      "Iteration 730, loss = 0.00143224\n",
      "Iteration 731, loss = 0.00144398\n",
      "Iteration 732, loss = 0.00143384\n",
      "Iteration 733, loss = 0.00142549\n",
      "Iteration 734, loss = 0.00137798\n",
      "Iteration 735, loss = 0.00141260\n",
      "Iteration 736, loss = 0.00140704\n",
      "Iteration 737, loss = 0.00139334\n",
      "Iteration 738, loss = 0.00143282\n",
      "Iteration 739, loss = 0.00136814\n",
      "Iteration 740, loss = 0.00145167\n",
      "Iteration 741, loss = 0.00145525\n",
      "Iteration 742, loss = 0.00135928\n",
      "Iteration 743, loss = 0.00142881\n",
      "Iteration 744, loss = 0.00144737\n",
      "Iteration 745, loss = 0.00141502\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(20, 20), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 -> 100 -> 100 -> 1\n",
    "# 3 -> 2 -> 2 -> 1\n",
    "rede_neural_credit = MLPClassifier(max_iter=1500, verbose=True, tol=0.0000100,\n",
    "                                   solver = 'adam', activation = 'relu',\n",
    "                                   hidden_layer_sizes = (20,20))\n",
    "rede_neural_credit.fit(X_credit_treinamento, y_credit_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9afd31e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39cf9aa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_credit_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd03c15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9c7d82d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treinamento, y_credit_treinamento)\n",
    "cm.score(X_credit_teste, y_credit_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11d5e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      0.97      0.98        64\n",
      "\n",
      "    accuracy                           0.99       500\n",
      "   macro avg       0.99      0.98      0.99       500\n",
      "weighted avg       0.99      0.99      0.99       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40481502",
   "metadata": {},
   "source": [
    "## Base census - 81.53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b1392eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('census.pkl', 'rb') as f:  \n",
    "  X_census_treinamento, y_census_treinamento, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c10426cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_treinamento.shape, y_census_treinamento.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f9dbe00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4885, 108), (4885,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_census_teste.shape, y_census_teste.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13baa91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(108 + 1) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b38e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39330691\n",
      "Iteration 2, loss = 0.32669217\n",
      "Iteration 3, loss = 0.31458036\n",
      "Iteration 4, loss = 0.30763744\n",
      "Iteration 5, loss = 0.30176754\n",
      "Iteration 6, loss = 0.29854649\n",
      "Iteration 7, loss = 0.29511561\n",
      "Iteration 8, loss = 0.29282596\n",
      "Iteration 9, loss = 0.28983686\n",
      "Iteration 10, loss = 0.28743590\n",
      "Iteration 11, loss = 0.28537303\n",
      "Iteration 12, loss = 0.28296772\n",
      "Iteration 13, loss = 0.28142449\n",
      "Iteration 14, loss = 0.27898642\n",
      "Iteration 15, loss = 0.27827123\n",
      "Iteration 16, loss = 0.27538892\n",
      "Iteration 17, loss = 0.27452406\n",
      "Iteration 18, loss = 0.27174214\n",
      "Iteration 19, loss = 0.27015447\n",
      "Iteration 20, loss = 0.26915697\n",
      "Iteration 21, loss = 0.26764897\n",
      "Iteration 22, loss = 0.26656804\n",
      "Iteration 23, loss = 0.26389153\n",
      "Iteration 24, loss = 0.26230071\n",
      "Iteration 25, loss = 0.26094610\n",
      "Iteration 26, loss = 0.25964447\n",
      "Iteration 27, loss = 0.25817812\n",
      "Iteration 28, loss = 0.25606648\n",
      "Iteration 29, loss = 0.25549699\n",
      "Iteration 30, loss = 0.25431862\n",
      "Iteration 31, loss = 0.25334321\n",
      "Iteration 32, loss = 0.25123214\n",
      "Iteration 33, loss = 0.24985595\n",
      "Iteration 34, loss = 0.24860622\n",
      "Iteration 35, loss = 0.24737464\n",
      "Iteration 36, loss = 0.24693015\n",
      "Iteration 37, loss = 0.24496976\n",
      "Iteration 38, loss = 0.24384368\n",
      "Iteration 39, loss = 0.24267992\n",
      "Iteration 40, loss = 0.24081995\n",
      "Iteration 41, loss = 0.23974469\n",
      "Iteration 42, loss = 0.23738655\n",
      "Iteration 43, loss = 0.23726938\n",
      "Iteration 44, loss = 0.23645460\n",
      "Iteration 45, loss = 0.23505676\n",
      "Iteration 46, loss = 0.23340660\n",
      "Iteration 47, loss = 0.23271326\n",
      "Iteration 48, loss = 0.23130744\n",
      "Iteration 49, loss = 0.22979344\n",
      "Iteration 50, loss = 0.22925134\n",
      "Iteration 51, loss = 0.22787320\n",
      "Iteration 52, loss = 0.22649639\n",
      "Iteration 53, loss = 0.22637535\n",
      "Iteration 54, loss = 0.22555795\n",
      "Iteration 55, loss = 0.22394572\n",
      "Iteration 56, loss = 0.22414741\n",
      "Iteration 57, loss = 0.22210219\n",
      "Iteration 58, loss = 0.22063641\n",
      "Iteration 59, loss = 0.22045913\n",
      "Iteration 60, loss = 0.21935118\n",
      "Iteration 61, loss = 0.21838328\n",
      "Iteration 62, loss = 0.21761540\n",
      "Iteration 63, loss = 0.21622617\n",
      "Iteration 64, loss = 0.21610702\n",
      "Iteration 65, loss = 0.21509910\n",
      "Iteration 66, loss = 0.21487945\n",
      "Iteration 67, loss = 0.21278195\n",
      "Iteration 68, loss = 0.21308505\n",
      "Iteration 69, loss = 0.21244729\n",
      "Iteration 70, loss = 0.21111383\n",
      "Iteration 71, loss = 0.21037569\n",
      "Iteration 72, loss = 0.21011816\n",
      "Iteration 73, loss = 0.20847778\n",
      "Iteration 74, loss = 0.20952663\n",
      "Iteration 75, loss = 0.20612503\n",
      "Iteration 76, loss = 0.20689228\n",
      "Iteration 77, loss = 0.20641810\n",
      "Iteration 78, loss = 0.20539707\n",
      "Iteration 79, loss = 0.20567092\n",
      "Iteration 80, loss = 0.20376238\n",
      "Iteration 81, loss = 0.20419821\n",
      "Iteration 82, loss = 0.20304753\n",
      "Iteration 83, loss = 0.20199303\n",
      "Iteration 84, loss = 0.20137802\n",
      "Iteration 85, loss = 0.20185656\n",
      "Iteration 86, loss = 0.20068197\n",
      "Iteration 87, loss = 0.19916301\n",
      "Iteration 88, loss = 0.19947017\n",
      "Iteration 89, loss = 0.19838775\n",
      "Iteration 90, loss = 0.19889937\n",
      "Iteration 91, loss = 0.19787314\n",
      "Iteration 92, loss = 0.19673229\n",
      "Iteration 93, loss = 0.19641306\n",
      "Iteration 94, loss = 0.19626742\n",
      "Iteration 95, loss = 0.19535829\n",
      "Iteration 96, loss = 0.19562160\n",
      "Iteration 97, loss = 0.19469156\n",
      "Iteration 98, loss = 0.19370124\n",
      "Iteration 99, loss = 0.19325865\n",
      "Iteration 100, loss = 0.19366123\n",
      "Iteration 101, loss = 0.19264297\n",
      "Iteration 102, loss = 0.19232343\n",
      "Iteration 103, loss = 0.19352391\n",
      "Iteration 104, loss = 0.19152539\n",
      "Iteration 105, loss = 0.19012224\n",
      "Iteration 106, loss = 0.19084483\n",
      "Iteration 107, loss = 0.18951410\n",
      "Iteration 108, loss = 0.18868796\n",
      "Iteration 109, loss = 0.18945080\n",
      "Iteration 110, loss = 0.18936684\n",
      "Iteration 111, loss = 0.18880640\n",
      "Iteration 112, loss = 0.18718903\n",
      "Iteration 113, loss = 0.18724534\n",
      "Iteration 114, loss = 0.18676094\n",
      "Iteration 115, loss = 0.18719093\n",
      "Iteration 116, loss = 0.18514470\n",
      "Iteration 117, loss = 0.18636078\n",
      "Iteration 118, loss = 0.18518099\n",
      "Iteration 119, loss = 0.18349847\n",
      "Iteration 120, loss = 0.18354186\n",
      "Iteration 121, loss = 0.18405388\n",
      "Iteration 122, loss = 0.18400464\n",
      "Iteration 123, loss = 0.18385145\n",
      "Iteration 124, loss = 0.18339495\n",
      "Iteration 125, loss = 0.18296139\n",
      "Iteration 126, loss = 0.18157721\n",
      "Iteration 127, loss = 0.18021018\n",
      "Iteration 128, loss = 0.17996937\n",
      "Iteration 129, loss = 0.18039983\n",
      "Iteration 130, loss = 0.18080526\n",
      "Iteration 131, loss = 0.18005494\n",
      "Iteration 132, loss = 0.17938162\n",
      "Iteration 133, loss = 0.17872835\n",
      "Iteration 134, loss = 0.17897766\n",
      "Iteration 135, loss = 0.17763005\n",
      "Iteration 136, loss = 0.17865935\n",
      "Iteration 137, loss = 0.17737225\n",
      "Iteration 138, loss = 0.17745302\n",
      "Iteration 139, loss = 0.17678868\n",
      "Iteration 140, loss = 0.17644626\n",
      "Iteration 141, loss = 0.17451590\n",
      "Iteration 142, loss = 0.17586965\n",
      "Iteration 143, loss = 0.17705294\n",
      "Iteration 144, loss = 0.17609561\n",
      "Iteration 145, loss = 0.17549992\n",
      "Iteration 146, loss = 0.17449213\n",
      "Iteration 147, loss = 0.17338552\n",
      "Iteration 148, loss = 0.17296975\n",
      "Iteration 149, loss = 0.17183854\n",
      "Iteration 150, loss = 0.17333844\n",
      "Iteration 151, loss = 0.17244327\n",
      "Iteration 152, loss = 0.17311401\n",
      "Iteration 153, loss = 0.17174044\n",
      "Iteration 154, loss = 0.17145152\n",
      "Iteration 155, loss = 0.17167210\n",
      "Iteration 156, loss = 0.17167354\n",
      "Iteration 157, loss = 0.17064047\n",
      "Iteration 158, loss = 0.17096043\n",
      "Iteration 159, loss = 0.17058697\n",
      "Iteration 160, loss = 0.16952070\n",
      "Iteration 161, loss = 0.16902328\n",
      "Iteration 162, loss = 0.16976149\n",
      "Iteration 163, loss = 0.16929008\n",
      "Iteration 164, loss = 0.16920418\n",
      "Iteration 165, loss = 0.16783071\n",
      "Iteration 166, loss = 0.16897952\n",
      "Iteration 167, loss = 0.16681886\n",
      "Iteration 168, loss = 0.16710842\n",
      "Iteration 169, loss = 0.16713224\n",
      "Iteration 170, loss = 0.16714650\n",
      "Iteration 171, loss = 0.16887705\n",
      "Iteration 172, loss = 0.16611052\n",
      "Iteration 173, loss = 0.16466279\n",
      "Iteration 174, loss = 0.16377777\n",
      "Iteration 175, loss = 0.16361193\n",
      "Iteration 176, loss = 0.16464601\n",
      "Iteration 177, loss = 0.16591250\n",
      "Iteration 178, loss = 0.16251546\n",
      "Iteration 179, loss = 0.16512957\n",
      "Iteration 180, loss = 0.16297151\n",
      "Iteration 181, loss = 0.16339938\n",
      "Iteration 182, loss = 0.16305619\n",
      "Iteration 183, loss = 0.16158499\n",
      "Iteration 184, loss = 0.16239891\n",
      "Iteration 185, loss = 0.16256383\n",
      "Iteration 186, loss = 0.16173925\n",
      "Iteration 187, loss = 0.16080893\n",
      "Iteration 188, loss = 0.16419764\n",
      "Iteration 189, loss = 0.16178762\n",
      "Iteration 190, loss = 0.16136074\n",
      "Iteration 191, loss = 0.15938458\n",
      "Iteration 192, loss = 0.16034768\n",
      "Iteration 193, loss = 0.15916168\n",
      "Iteration 194, loss = 0.16061861\n",
      "Iteration 195, loss = 0.15980709\n",
      "Iteration 196, loss = 0.15890976\n",
      "Iteration 197, loss = 0.15823959\n",
      "Iteration 198, loss = 0.15863144\n",
      "Iteration 199, loss = 0.15778791\n",
      "Iteration 200, loss = 0.15859160\n",
      "Iteration 201, loss = 0.15909059\n",
      "Iteration 202, loss = 0.15827889\n",
      "Iteration 203, loss = 0.15796914\n",
      "Iteration 204, loss = 0.15788482\n",
      "Iteration 205, loss = 0.15763952\n",
      "Iteration 206, loss = 0.15596951\n",
      "Iteration 207, loss = 0.15790748\n",
      "Iteration 208, loss = 0.15820135\n",
      "Iteration 209, loss = 0.15696598\n",
      "Iteration 210, loss = 0.15628987\n",
      "Iteration 211, loss = 0.15567916\n",
      "Iteration 212, loss = 0.15581546\n",
      "Iteration 213, loss = 0.15870435\n",
      "Iteration 214, loss = 0.15758878\n",
      "Iteration 215, loss = 0.15481975\n",
      "Iteration 216, loss = 0.15491380\n",
      "Iteration 217, loss = 0.15516828\n",
      "Iteration 218, loss = 0.15476682\n",
      "Iteration 219, loss = 0.15475227\n",
      "Iteration 220, loss = 0.15364042\n",
      "Iteration 221, loss = 0.15289668\n",
      "Iteration 222, loss = 0.15330417\n",
      "Iteration 223, loss = 0.15357760\n",
      "Iteration 224, loss = 0.15326700\n",
      "Iteration 225, loss = 0.15295747\n",
      "Iteration 226, loss = 0.15253062\n",
      "Iteration 227, loss = 0.15276504\n",
      "Iteration 228, loss = 0.15487849\n",
      "Iteration 229, loss = 0.15144086\n",
      "Iteration 230, loss = 0.15213972\n",
      "Iteration 231, loss = 0.15187653\n",
      "Iteration 232, loss = 0.15177919\n",
      "Iteration 233, loss = 0.15039646\n",
      "Iteration 234, loss = 0.15048807\n",
      "Iteration 235, loss = 0.15270206\n",
      "Iteration 236, loss = 0.15212419\n",
      "Iteration 237, loss = 0.15060016\n",
      "Iteration 238, loss = 0.15071338\n",
      "Iteration 239, loss = 0.14974234\n",
      "Iteration 240, loss = 0.14947806\n",
      "Iteration 241, loss = 0.15077613\n",
      "Iteration 242, loss = 0.15128107\n",
      "Iteration 243, loss = 0.14984377\n",
      "Iteration 244, loss = 0.14935552\n",
      "Iteration 245, loss = 0.14877644\n",
      "Iteration 246, loss = 0.14936062\n",
      "Iteration 247, loss = 0.14874681\n",
      "Iteration 248, loss = 0.14869517\n",
      "Iteration 249, loss = 0.14888842\n",
      "Iteration 250, loss = 0.14938880\n",
      "Iteration 251, loss = 0.15098001\n",
      "Iteration 252, loss = 0.14855588\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 253, loss = 0.14801302\n",
      "Iteration 254, loss = 0.14915098\n",
      "Iteration 255, loss = 0.15136029\n",
      "Iteration 256, loss = 0.14799969\n",
      "Iteration 257, loss = 0.14666682\n",
      "Iteration 258, loss = 0.14719579\n",
      "Iteration 259, loss = 0.14580410\n",
      "Iteration 260, loss = 0.14676704\n",
      "Iteration 261, loss = 0.14659931\n",
      "Iteration 262, loss = 0.14687267\n",
      "Iteration 263, loss = 0.14548737\n",
      "Iteration 264, loss = 0.14540162\n",
      "Iteration 265, loss = 0.14578037\n",
      "Iteration 266, loss = 0.14625764\n",
      "Iteration 267, loss = 0.14647069\n",
      "Iteration 268, loss = 0.14745386\n",
      "Iteration 269, loss = 0.14642905\n",
      "Iteration 270, loss = 0.14536738\n",
      "Iteration 271, loss = 0.14698003\n",
      "Iteration 272, loss = 0.14694044\n",
      "Iteration 273, loss = 0.14547902\n",
      "Iteration 274, loss = 0.14510522\n",
      "Iteration 275, loss = 0.14500245\n",
      "Iteration 276, loss = 0.14446148\n",
      "Iteration 277, loss = 0.14410836\n",
      "Iteration 278, loss = 0.14323688\n",
      "Iteration 279, loss = 0.14499664\n",
      "Iteration 280, loss = 0.14496077\n",
      "Iteration 281, loss = 0.14463866\n",
      "Iteration 282, loss = 0.14414714\n",
      "Iteration 283, loss = 0.14400970\n",
      "Iteration 284, loss = 0.14370850\n",
      "Iteration 285, loss = 0.14373697\n",
      "Iteration 286, loss = 0.14371833\n",
      "Iteration 287, loss = 0.14307106\n",
      "Iteration 288, loss = 0.14262026\n",
      "Iteration 289, loss = 0.14298423\n",
      "Iteration 290, loss = 0.14195197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miguel/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:619: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 108 -> 55 -> 55 -> 1\n",
    "rede_neural_census = MLPClassifier(verbose=True, max_iter = 1000, tol=0.000010,\n",
    "                                  hidden_layer_sizes = (55,55))\n",
    "rede_neural_census.fit(X_census_treinamento, y_census_treinamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a76c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' >50K'],\n",
       "      dtype='<U6')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previsoes = rede_neural_census.predict(X_census_teste)\n",
    "previsoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ebe88e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' <=50K', ' <=50K', ' <=50K', ..., ' <=50K', ' <=50K', ' <=50K'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_census_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cdf1fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167860798362334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_census_teste, previsoes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3533616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8167860798362334"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_treinamento, y_census_treinamento)\n",
    "cm.score(X_census_teste, y_census_teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ee081a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.88      0.88      3693\n",
      "        >50K       0.63      0.61      0.62      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.75      0.75      0.75      4885\n",
      "weighted avg       0.82      0.82      0.82      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e068b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
